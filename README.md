# üìå Overview

`corrdiff_input` is a data‚Äìpreparation pipeline for constructing CorrDiff-ready Zarr datasets from multiple atmospheric data sources, including:
- ERA5 (ECMWF ReAnalysis v5)
- TReAD (Taiwan ReAnalysis Downscaling Data)
- TaiESM 3.5 km / TaiESM 100 km (Taiwan Earth System Model, for SSP climate scenarios)

![image](graphic/infograph.png)

The pipeline:
1. Loads raw NetCDF files
2. Regrids all datasets onto a unified WRF-style reference grid
3. Normalizes, centers & stacks channels into CorrDiff-compatible tensors
4. Outputs the consolidated dataset as a compressed Zarr store

More details in [intro deck](graphic/intro_deck.pdf).

# üóÇ Features
‚úÖ Unified Reference Grid

Creates or loads a consistent **208√ó208 WRF-style grid**.

‚úÖ Multi-source Dataset Loading

Supports:
- ERA5 surface (SFC)
- ERA5 pressure-level (PRS)
- TReAD high-resolution fields
- TaiESM 3.5 km / 100 km (SSP scenarios)

‚úÖ Regridding & Spatial Alignment

Built-in utilities for:
- Bilinear interpolation
- Nearest-cell extrapolation
- Optional custom grid generation

‚úÖ CorrDiff-Ready Tensor Construction

Creates:
- `cwb_*` (TReAD / TaiESM 3.5km)
- `era5_*` (ERA5 / TaiESM 100km)
- Associated metadata: center, scale, variable names, masks

‚úÖ Zarr Export & Validation

- Outputs compressed Zarr datasets
- Tools for merging, slicing, and inspecting Zarr stores

# üì¶ Installation

Before using the project, install the required dependencies:

```
conda env create -f env/corrdiff_input.yml
```

# üöÄ Usage

## 1Ô∏è‚É£ Generate a Single CorrDiff Dataset

- CWA mode (TReAD + ERA5):
  ```
  python src/corrdiff_datagen.py <start_date> <end_date>
  ```
  Example: `python src/corrdiff_datagen.py 20180101 20180131`

- SSP mode (TaiESM 3.5 km + 100 km):
  ```
  python src/corrdiff_datagen.py <start_date> <end_date> <ssp_level>
  ```
  Example: `python src/corrdiff_datagen.py 20180101 20180131 ssp585`

This will:
- Load and regrid the input datasets
- Construct CorrDiff-formatted tensors
- Saves output to
  ```
  corrdiff_dataset_<start_date>_<end_date>.zarr
  # or
  corrdiff_dataset_<start_date>_<end_date>_<ssp_level>.zarr
  ```

### üîç Debugging Regridding Artifacts
Enable NetCDF dumps in `src/corrdiff_datagen.py`:
```
DEBUG = True  # Set to True to enable debugging
```

This writes:
```
nc_dump/<start_date>_<end_date>/
   highres_pre_regrid.nc
   highres_post_regrid.nc
   lowres_pre_regrid.nc
   lowres_post_regrid.nc
```

## 2Ô∏è‚É£ Generate Multi-Year Datasets (avoid OOM)

For long time ranges (> 8 years):

### CWA (TReAD + ERA5)
```
./datagen_n_merge.sh <start_date> <end_date>
```

### SSP (TaiESM)

__Single SSP level__

Run for a specific SSP scenario (`historical` / `ssp126` / `ssp245` / `ssp370` / `ssp585`):
```
./datagen_n_merge.sh <start_date> <end_date> <ssp_level>
```

__All non-historical SSP levels__

Run for all supported future SSP scenarios (`ssp126`, `ssp245`, `ssp370`, and `ssp585`):
```
./datagen_n_merge.sh <start_date> <end_date> all
```

The script:
- Splits the time range into periods <= 8 years
- Generates Zarr files per period
- Merges them into a single consolidated Zarr store

The reason is to avoid OOM on BIG server given dataset with > 8-year time range.

## 3Ô∏è‚É£ Inspect / Filter / Merge Zarr Files

### Inspect structure and preview slices

```
python src/helpers/dump_zarr.py <input_zarr_file>
```

### Filter data by dates

Revise file paths in `src/helpers/filter_zarr.py`, and run:
```
python src/helpers/filer_zarr.py
```

### Merge multiple Zarr files

Put all `corrdiff_*.zarr` files under `./`, and run:
```
python src/helpers/merge_zarr.py
```

## 4Ô∏è‚É£ Generate Reference Grid

### Create a new 208√ó208 WRF-style grid:
```
cd ref_grid
python generate_wrf_coord.py
```

### Adjust grid size:

1. Modify `ny` and `nx` in `generate_wrf_coord.py`:

```
ny, nx = 208, 208               # Desired grid dimensions
```

2. Revise `*_REF_GRID` in `src/data_builder.py` acccordingly:

```
CWA_REF_GRID = "../ref_grid/wrf_208x208_grid_coords.nc"
SSP_REF_GRID = "../ref_grid/ssp_208x208_grid_coords.nc"
```

## 5Ô∏è‚É£ Verify Dataset Format

### Verify low-resolution dataset file format

Put SFC / PRS `.nc` files of the same time range under the same folder, and run:
```
python src/helpers/verify_lowres_fmt.py sfc <sfc_files_folder>
# or
python src/helpers/verify_lowres_fmt.py prs <prs_files_folder>
```

### Verify `time` coordinate consistency in dataset

```
python src/helpers/verify_time_coord.py <top_folder>
```

# üìÇ Project Structure

## Prepare NetCDF files (for local testing only)

### CWA mode
- Put TReAD file below under `data/tread/`.
  - `wrfo2D_d02_201801.nc`
- Put ERA5 files below under `data/era5/`.
  - `ERA5_PRS_*_201801_r1440x721_day.nc`
  - `ERA5_SFC_*_201801_r1440x721_day.nc`

### SSP mode
- Put TaiESM 3.5 km file below under `data/taiesm3p5/`.
  - `TaiESM1-WRF_tw3.5_ssp126_wrfday_d01_201801.nc`
- Put TaiESM 100 km below under `data/taiesm100/SFC` and `data/taiesm100/PRS` respectively.
  - `TaiESM1_ssp126_r1i1p1f1_*_EA_201801_day.nc`

## Example
```
üì¶ corrdiff_input
 ‚î£ üìÇ env/      # YML files to create conda environment
 ‚î£ üìÇ data/     # Input data (NetCDF files of low- and high-resolution datasets)
   ‚î£ üìÇ tread/
     ‚îó üìú wrfo2D_d02_201801.nc
   ‚î£ üìÇ era5/
     ‚î£ üìú ERA5_SFC_*_201801_r1440x721_day.nc
     ‚îó üìú ERA5_PRS_*_201801_r1440x721_day.nc
   ‚î£ üìÇ taiesm3p5/
     ‚îó üìú TaiESM1-WRF_tw3.5_ssp126_wrfday_d01_201801.nc
   ‚î£ üìÇ taiesm100/
     ‚î£ üìÇ SFC/
       ‚îó üìú TaiESM1_ssp126_r1i1p1f1_*_EA_201801_day.nc
     ‚îó üìÇ PRS/
       ‚îó üìú TaiESM1_ssp126_r1i1p1f1_*_EA_201801_day.nc
   ‚îó üìÇ extreme_dates/              # Extreme precipitation dates
 ‚î£ üìÇ ref_grid/
   ‚î£ üìú generate_wrf_coord.py       # REF grid generation script
   ‚î£ üìú TReAD_wrf_d02_info.nc       # TReAD grid used to generate REF grid
   ‚î£ üìú wrf_208x208_grid_coords.nc  # CWA 208x208 REF grid
   ‚î£ üìú TAIESM_tw3.5km_coord2d.nc   # TaiESM 3.5 km grid used to generate REF grid
   ‚îó üìú ssp_208x208_grid_coords.nc  # SSP 208x208 REF grid
 ‚î£ üìÇ src/
   ‚î£ üìÇ helpers/              # Zarr utilities & Data validators
   ‚î£ üìÇ data_loader/          # TReAD / ERA5 / TaiESM data loaders
   ‚î£ üìú corrdiff_datagen.py   # Zarr generation script
   ‚î£ üìú data_builder.py       # Low- and high-resolution dataset builder script
   ‚îó üìú tensor_fields.py      # Tensor fields constructor for Zarr
 ‚î£ üìú datagen_n_merge.sh      # Shell script to create and merge datasets
 ‚îó üìú README.md               # Project documentation
```

# üìò Script Overview

### üîπ `src/corrdiff_datagen.py`

Main driver:
- Loads datasets
- Regrids to REF grid
- Builds CorrDiff-ready tensors
- Saves to Zarr

### üîπ `src/data_builder.py`

CorrDiff data assembly module:
- Loads REF grid
- Builds low- and high-resolution datasets
- Converts datasets into CorrDiff-ready tensors

### üîπ `src/tensor_fields.py`

Tensor creation logic:
- Stacks channels
- Normalizes data (center/scale)
- Creates metadata

### üîπ `src/data_loaders/`

- Loads datasets for:
  - TReAD / ERA5
  - TaiESM 3.5 km / 100 km
- Provides utilities:
  - Regridding
  - File data format validation

### üîπ `src/helpers/`

Post-processing & debugging helpers -

#### `*_zarr.py`

- Inspects single Zarr file
- Merges multiple Zarr files
- Filters data in Zarr by dates

#### `verify_*.py`

- Validates low-resolution data file format
- Validates data file `time` coordinate consistency

#### `geo_region.py`

- Plots geographic region

### üîπ `ref_grid/`

WRF-style reference grid creation.
- Allows easy grid size customization (e.g., 208x208 -> any `ny` x `nx`)
- Supports CWA and SSP modes for different configurations

# üéØ Why Use This Project?

- ‚úÖ Automates multi-source climate dataset preparation
- ‚úÖ Ensures spatial & temporal consistency
- ‚úÖ Outputs CorrDiff-ready tensors
- ‚úÖ Handles large datasets via Dask & Zarr
- ‚úÖ Provides debugging & inspection tools
- ‚úÖ Extensible to new climate models

# ü§ù Contributing

PRs and issues are welcome! If integrating new datasets or grids, please document them clearly.
